The U.S. economy added an estimated 313,000 jobs in February, keeping the unemployment rate at 4.1 percent, where it’s held steady for the last five months. And the good news is, experts agree, that it still has room to improve. But there’s even better news if you look closely: The lowest unemployment rate most economists think we can reach is actually still too high. Within economics, there’s a concept called “full employment.” There’s a debate within the field over what that means in terms of the unemployment rate. In the New York Times, Neil Irwin suggests 3.5 percent may be the most realistic level we can hope for; Alan Blinder, an economist Irwin quotes, gives a range with 2.5 percent as his low-end estimate. There’s good reason to believe, however, that we should strive for the unemployment rate to be well under 2 percent, and potentially even lower. The idea behind full employment is that there is some “natural” level of unemployment caused by basic “frictions” in the job market, like temporary layoffs and job searches. Get any lower than this “natural” rate, the theory goes, and workers will have too much bargaining power relative to employers. They’ll negotiate higher wages than their productivity warrants, employers will raise prices in response, workers will negotiate higher wages again, prices will rise again, and we’ll be caught in a difficult-to-stop cycle of accelerating inflation. Terrible, right!? If you’re not scared, I don’t blame you. Nothing remotely close to the dynamic outlined above has surfaced in almost four decades. One might cite the late 1990s as an exception, as unemployment in the 4 percent range did correspond with rising wages, but it didn’t produce the kind of inflationary spiral that economists often worry about. More recently, unemployment has been below the Federal Reserve’s “natural rate” estimate and inflation hasn’t budged. So why are economists so wrong about full employment? One reason is that the labor market just doesn’t work the way the classic models suggest it does, especially since union membership and federal labor standards have been eroded by years of corporate attacks. Wages are not set, as in full employment theory, by some pristine equilibrium between supply and demand; workers are not paid their marginal contribution to their firms’ output. In the real world, pay is determined by a negotiation between workers and employers, and employers just typically have a lot more negotiating power than their employees do. While there’s evidence that lower unemployment rates do give workers some power (by making employers less sure they can replace workers who ask for raises), it’s not enough power for workers to secure raises that reflect their work effort. The evidence that higher wages necessarily lead to price hikes isn’t exactly rock-solid, either. The other big problem with theories of full employment is that they’re based on historical data. Even in periods of very strong labor demand, there is logically some level of unemployment that is truly unavoidable because people are hired and fired, or change employers or quit, all of which involves transition time between one job and the next. But there’s no reason to assume that we’ve ever been particularly close to that level of unemployment. [Yes, the robots will steal our jobs. And that’s fine.] Consider, for example, that the black unemployment rate has been about twice as high as the white unemployment rate since the Bureau of Labor Statistics began publishing black unemployment data in 1972, and that even February’s 6.9 percent black unemployment rate, which is close to December’s record low, was higher than the white unemployment rate in 80 percent of the months since then. Economists seem to take the discrepancy between the black and white unemployment rates as a given, but they’re actually a reflection of institutional racism. Very little of the long-standing 2-1 ratio can be attributed to differential rates of educational attainment among black and white populations (which are themselves the product of centuries of discrimination); we know that millions of Americans have historically been denied jobs on the basis of their racial identities alone. It is most certainly not the case that the “natural” rate of unemployment for white workers is in the 3.5 percent range while the “natural” rate of unemployment for black workers is twice as high. What’s needed is a benchmark full employment rate that accounts for discrimination and is a reasonable proxy for the true level of frictional unemployment in the economy. The unemployment rate of white college graduates — workers unlikely to be discriminated against on the basis of race and possessing the credential many employers use to weed out the workers they want to consider hiring — i.e., a college degree — fits the bill. If you’re a white college graduate and you end up unemployed, you’re likely to find a new opportunity reasonably soon. (Though even for many white college graduates, of course, finding a good job is hardly a piece of cake). Our economy can and should work the same way for other workers, but it currently doesn’t. [The economy on the eve of the tax cut] One might contend that college graduates are more productive than workers without college degrees and that their unemployment rates should be lower than the rates for their less-educated counterparts. But high levels of educational attainment aren’t actually a prerequisite for many of the jobs in our economy. Employer claims that they can’t find workers with the skills they need are much less often true than many people think, so there’s not much basis behind the idea that employers can’t afford to hire and train non-college-educated workers. What’s a good unemployment rate to aim for, then? The white college graduate unemployment rate was 1.7 percent during the late 1990s, the last time many economists think we were at full employment. So let’s stop suggesting arbitrarily high target rates of unemployment and start focusing on making 1.7 percent unemployment or lower the new normal — for everyone.