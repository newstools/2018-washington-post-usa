 [This story has been optimized for offline reading on our apps. For a richer experience, you can find the full version of this story here. An Internet connection is required.] Each time I commanded a unit during my years in the Army (except in war zones), I ran competitions to promote excellence in physical fitness, and buddy-held sit-ups — with your fingers behind your head and your feet under a friend or a heavy object — were part of the program. The competitions also typically included a two-to-four-mile run, pull-ups, dips, push-ups and various other exercises. I participated in dozens of these competitions myself and practiced for them thousands of times: Leaders need to lead by example, after all. With decades of practice, I frequently held the sit-up record, even in units with thousands of soldiers. My personal best was 143 sit-ups in two minutes. For some reason, I never associated — or refused to associate — the lower-back and neck pain that I often experienced during weeks of multiple competitions with the performance of sit-ups. But as research made the toll sit-ups take increasingly clear, I finally ended my denial when I was a three-star commander of the Combined Arms Center at Fort Leavenworth in Kansas. There, we replaced sit-ups with the “eagle ab” (named for the “Screaming Eagles” of the 101st Airborne), a very tough test of stomach, core and arm muscles that involves hanging fully extended from a pull-up bar with hands facing away, curling your body up to touch your shoelaces to the bar and then returning to a dead-hang position. Another challenging option was the “devil ab” (named for the Devils in Baggy Pants, a brigade of the 82nd Airborne): Lie on your back with your legs extended, your feet several inches off the ground and fingertips touching gently behind your ears. Then bring your knees up while your upper body curls (not as in a crunch, but with the body balanced with knees bent and feet still a few inches off the ground) before returning to the starting position. These two exercises, and others, can strengthen your core and abs without the strain on the neck and lower back of buddy-held sit-ups. It is time to sweep buddy-held sit-ups into the ash heap of physical fitness.  It’s deep into the second half of a critical, win-or-go-home game, which is the only kind there is in the NCAA men’s basketball tournament. Texas A&M is hoping to knock off the favored North Carolina Tar Heels. At question is the relative flagrancy of a foul committed. Should the punishment be one foul shot or two? Scrutinizing this Talmudic matter are the game’s referees, huddled by the sideline, peering into a TV monitor. The process goes on for nearly four minutes. Every breath in the arena is bated, or would be bated, but for one fact: The Aggies are ahead, 73-53. Who cares what the call is? The game is over. Of the 1,312 calls that were reviewed via video replay in Major League Baseball games last year, the site MLB Replay Stats reports that 47.4 percent were overturned. The National Football League, according to the website Football Zebras, posted a similar video replay reversal rate, 45.3 percent. Some of the reversals were defensible, others inexplicable. But what is the cost of “getting the call right”? Instant-replay review and the way it’s conducted have turned basketball and football into slogs. The promise of sports to deliver a just result is so great that we make too many excuses for the fact that, as a practical matter, replay reviews almost never add to the entertainment product. They are the new static in a sports broadcast, the enemy of compelling TV and great athletic feats. The NCAA women’s final ended on a shot by Arike Ogunbowale to win the championship for Notre Dame. But for minutes, Notre Dame’s celebration was suppressed so the video could be rewound and picked over, all to determine that 0.1 second was left on the game clock: no time to make a shot that could have tied the contest, but plenty of time to dampen the emotion and persuade TV viewers to change the channel. Twitter: @pescami When Martin Luther King Jr. spoke of a world where his children wouldn’t be “judged by the color of their skin but by the content of their character,” he was asking Americans to not let differences affect how they treat one another — not to pretend differences don’t exist. Somehow, his wish became a mantra that we should have a “colorblind” society. To that, King would offer a very color-conscious response: Negro, please. Hardly a conversation on race occurs today without someone claiming they don’t see color — Ivanka Trump called her father “colorblind”; Eric Trump once said, “My father sees one color: green.” Politicians seek race-neutral fixes to race-specific challenges like racial economic inequality — during his presidential run, Sen. Bernie Sanders (I-Vt.) was upbraided by the writer Ta-Nehisi Coates over his opposition to reparations. The term “post-racial” briefly caught on in the early days of Barack Obama’s presidency. The irony: We’re a nation that exalts hard work, but when it comes to addressing racial inequality, we’re lazy. One study found that minorities in the United States are segregated to a greater degree than in similar European countries. Black Americans fall below the national median in nearly every socioeconomic indicator: employment, wealth, education level, health outcomes. Latinos, too, fall below the median, and remain a scapegoat for the cultural anxieties shaping intolerant immigration policies. Asian Americans confront a double-edged “model minority” stereotype. Since the causes of racial inequality are color-conscious, the remedies must be, as well. To fulfill the nation’s promise, we have to see color and see each other as equally American. Only then can we dismantle the structural barriers preventing the United States from living its creed. Twitter: @DrTedJ  My kindergarten teacher, Mrs. Granberry, sat her students around a big communal table where we could all see one another and be supervised. There was yelling, commotion and general childlike chaos. The same things happen in an open office — only we’re grown-up professionals with deadlines, personal lives and a basic human need for some privacy. Adults are not immune to distractions; many of us welcome them. So the open space may reduce overhead — 70 percent of U.S. businesses use these kinds of workspaces — but it crushes performance. Research has shown that distractions in the communal corral cut productivity by 15 to 28 percent. Plus, sharing a space makes us sick, just like the kindergarten Petri dish: Studies show we’re twice as likely to get ill, leading to more absences and less work. When my ad agency moved from a cubicle/shared-private design to an open concept, I immediately felt an effect on my work. I could hardly get anything done during business hours as people stopped by with questions (they used to email) and lured each other into boisterous debates about which character might get offed on “The Walking Dead.” It’s like being at recess all day, even though you still have meetings and deliverables. Along with the cost savings, the C-suite loves to proclaim that open spaces promote camaraderie. But my rapport with co-workers was fantastic before we relocated to the great wall-less wonder. Some Apple employees are threatening to quit over their open “spaceship” headquarters. A main complaint many workers cite about such spaces is too much noise, which also elevates stress levels. Instead of joining group discussions, some employees opt for headphone-huddles to thwart the clamor and get work done; some companies even supply headphones. I’m here to report that they don’t work. I bought an obnoxious blue pair to signal to colleagues: “Do not disturb.” Guess what? They disturbed. After all, they could still see me. Twitter: @wearyourwholecloset Experts routinely warn people not to share their Social Security numbers. Yet whether applying for a loan, visiting a doctor or renting an apartment, consumers must frequently turn over this sensitive information to help organizations authenticate their identity. These everyday requests lead to stockpiles of Social Security numbers in insecure databases — rich targets for hackers like the ones who stole 148 million profiles in last year’s Equifax breach. Identity thieves buy the data and use it to fraudulently obtain tax refunds, take out loans and open credit card accounts. The best way to end this would be for Congress to ban the use of Social Security numbers to identify people. That was never the numbers’ intended purpose: For more than 25 years, the words “not for identification” were even printed on the bottom of Social Security cards. If these numbers couldn’t be used to commit financial crimes, that would meaningfully lower the stakes of many data breaches. Instead, Congress should expand the National Strategy for Trusted Identities in Cyberspace, an initiative started by the Commerce Department to create a secure electronic ID system that individuals could use for government and commercial applications. It could take the form of physical cards or digital certificates installed in mobile apps, which people could use to prove their identity to online services — to sign legal documents, open bank accounts and even confirm they are old enough to order alcohol online. Unlike past proposals for a national ID, these electronic IDs would be voluntary. And unlike Social Security numbers, this system would be designed from the bottom up to withstand today’s cyberthreats and unlock new opportunities for digital commerce. For example, using these IDs would require both a security token, such as a physical smart card, and a password or biometric, such as a fingerprint — making fraud significantly more difficult. Let’s put Social Security numbers in a lockbox and throw away the key. Twitter: @castrotech  I’m sorry, Officer. I did it. I killed them all. Golf was the first to go. It just seemed so stodgy and slow. No one born after 1982 was going to miss it. Next came bars of soap, which had outlived their usefulness — liquid soap is just so much easier, don’t you think? I killed department stores, too, because I didn’t like the format and presentation of the merchandise, and besides, I’d rather get my jeans from a stand-alone store of a brand I love. And I killed “breastaurants,” like Hooters, but we can all agree they had it coming. I didn’t act alone. All the other millennials were my accomplices. Usually we didn’t even plan to kill something — it just happened, and then we read about it later in a hyperbolic news story. Nylon magazine explained “How Millennials Killed J. Crew,” while Fortune described how “Millennials Are Killing Lunch.” We killed traditional gyms by neglect, because we went to studio classes like Barre more often. We killed cereal because a granola bar is more portable. We killed fabric softener and dinner dates and, somehow, doorbells. People have blamed us for killing homeownership, but that one was self-defense — we were being bludgeoned by student loan debt. And it’s not like we’re the only ones with blood on our hands: Previous generations killed the railroad industry, diners  and, ahem, the environment. But one murder for which we millennials would have no remorse is killing the phrase “Millennials killed ___.” We will dispatch it with blunt sarcasm and memes. Put that phrase in a headline now, and see how mercilessly you’re mocked on Twitter for your lazy generalizations about this generation. Yes, a lack of interest from young people helps explain why these products and industries are struggling. But maybe it’s because they haven’t changed with the times. Maybe we’re just a convenient scapegoat. Maybe it’s justifiable homicide. Twitter: @MauraJudkis  The Hollywood reboot — long a subject of complaint — has been remixed. Rehashing old hits with fresh faces is no longer enough: Audiences demand diversity, so studios now repackage women, people of color and other marginalized actors into roles previously portrayed only by straight, white men. “Yes! Finally! Representation!” we of the neglected classes cheer. Eh. Let’s not be so grateful for hand-me-downs. As a black female television writer, I think it’s fantastic for little girls to see that they, too, can grow up to be proton-pack-carrying Ghostbusters or thieves and con women, as in the upcoming all-female “Ocean’s 11” reboot. I’m overjoyed to see more “minority” actors working. But these secondhand reboot roles often seem like awkward attempts to put a Band-Aid on Hollywood’s very real problem of inclusion. And sometimes, these reboots are more than just ham-handed efforts to toss some diversity into a new moneymaking franchise. Take the “Roseanne” reboot. Why is it “important,” as Roseanne Barr says, that D.J. now have a black daughter because as a child he didn’t want to kiss a black girl? Are black TV children a reward for past prejudiced behavior? Can we at least get a “Family Matters” reboot so someone from the Winslows can check on her? My wish for the black actress playing Roseanne’s granddaughter is that, when she’s old enough to choose her own roles, she’ll have more pickings than an obvious diversity add-on in a show that originally would have considered her only as “girl D.J. doesn’t want to kiss.” Throwing the underserved a few rehashed roles when the vast majority of television shows and films are still helmed by the overserved? That’s not empowerment. That’s cheap window dressing over ugly systemic exclusion. Let’s get rid of “Drop a Random Character of Color In” and “The Minority Version Of” reboots. I don’t need to see “The DaVinci Codeswitch” with Michael B. Jordan instead of Tom Hanks. Instead, Hollywood should aim for true empowerment: letting women, people of color, the disabled and other overlooked creatives craft their own narratives. Twitter: @AngelaNissel Early in the seventh grade, my son, for reasons never voiced, waged a strike against homework. Neither stick nor carrot could shake his resolve, and much of what came to pass fell in the realm of the expected: emails from teachers, meetings with administrators, a procession of frustrated tutors. The unexpected part was that, after a few months, I began to see his point. “Why does he need to do all this stuff?” I found myself asking anyone who would listen. “Why does anyone?” There’s no good reason. Research has repeatedly shown that students who do homework perform no better than students who don’t. Even at the high school level, a significant 2012 study found, the amount of time spent on homework has zero impact on grades (which means that even the teachers who assign it don’t give it weight) and, at best, a teensy effect on standardized math and science test scores. As Alfie Kohn, author of “The Homework Myth,” wrote: “The better the research, the less likely one is to find any benefits from homework.” Which leaves us stuck in a feedback loop: teachers piling it on, students tossing it back and no one any the wiser. Perhaps some homework is “good” — reading a novel, say, before a class discussion — but so much more is a waste of time: the literary dioramas, the macaroni Colosseums, the chemistry crosswords, the spring break packets. Perhaps homework prepares a child for life — if that preparation is mainly learning to grit your teeth and perform pointless tasks, which is a skill we have all our days to master. Freed from homework, what might kids do? I dunno, stare into their phones? Play Far Cry 5? Braid each other’s hair? Nothing at all? The point is that kids deserve the same license to unwind that adults do. And parents deserve to be liberated from their uncompensated jobs as project managers. (Don’t get me started on science fair.) And teachers deserve to be liberated from their endless rounds of grading. When it comes down to it, the only reason we’re still assigning homework is because we always have. Twitter: @LouisBayard Once upon a time, you could go to a department store — say, Macy’s, Lord & Taylor or Bloomingdale’s — and buy a piece of quality clothing. These stores, and other brands, offered investment pieces: a little black dress, a statement bag, a luxury watch. Having these timeless pieces in our closets would make our lives effortless and chic. It was a given that the premium price tag meant the clothing was exceptional. I worked as a personal stylist for almost a decade, teaching clients how to shop, identify their personal style and build a wardrobe. Over the years, I started to notice a decline in the quality of clothing. The tipping point came, though, when I handed a client a Diane von Furstenberg dress to try on. The style was gorgeous, the color perfect, but when I turned the dress inside out, I saw uneven, sloppy stitching. Stitching is a clue to quality and can give a buyer a sense of how a garment will hold up over time. Seeing poor stitching in a brand known for quality cut deep. The problem isn’t that well-made garments don’t exist anymore. The problem is that quality is so darn inconsistent, even within a brand. A sped-up fashion calendar, globe-hopping manufacturing to meet production schedules and fashion houses playing musical chairs with their leadership have wreaked havoc on quality and led to a shopping environment that leaves consumers bewildered and exhausted. Add that to sartorially relaxed work environments, and chasing after those illusory investment pieces seems more and more like a charade. There’s no quick fix when it comes to building a perfect wardrobe — not even throwing a ton of cash at it. My advice? Do the best with the budget you have, buy for your actual life vs. one that’s imaginary, and always, always turn your clothes inside out to look at the stitching. Twitter: @truetostyle A few years back, a Wikipedia reader discovered that the site’s editors were systematically removing female writers from the “American novelists” page and placing them on a new list called “American women novelists.” There was no corresponding list for “American male novelists.” A male writer is just a writer, not a “man writer”; his work is not “men’s fiction.” Male is the default setting. Women remain the exception, the other, sequestered in a room of our own. “Women’s fiction” has been a catchall term for years, an ever-expanding tent big enough to cover anything written for women or by a woman, or consumed primarily by women. It’s Jodi Picoult and Tayari Jones. It’s light-as-souffle Sophie Kinsella. It’s Roxane Gay, deep-diving into issues of weight and sex and self-esteem. It’s romance (of course). It’s every book a woman ever wrote about a marriage or a family; every book ever to bear an Oprah’s Book Club sticker; every book with headless female bodies on pastel-colored covers. It’s also every book read by a female audience, which means that Dave Eggers and David Foster Wallace, Will Self and Jeffrey Eugenides and Michael Cunningham and Stephen King all write women’s fiction.  The label “women’s lit” is far from neutral. Some literary writers would rather die than have their books called women’s fiction. Some critics use the term to dismiss any book that smacks of sentimentality or to cut down any author who’s become too popular with the wrong kind of readers (remember the fuss over Donna Tartt’s “The Goldfinch,” which won the Pulitzer Prize but sold so well that some critics decided it couldn’t really be capital-L Literature). Male writers have traditionally had it both ways, taking on the topics of “women’s fiction” — marriage and family, domesticity and disappointment — while evading the label. If the appellation stands for everything that’s read by women or written by women, then it means nothing at all. Let’s toss it and call women’s works what they should have been called all along: books. Twitter: @jenniferweiner